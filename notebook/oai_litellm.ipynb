{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open-source Model Enablement with LiteLLM\n",
    "\n",
    "To implement the usage of LiteLLM and enable AutoGen to use open-sourced models while considering the context provided, we will address the points raised and create a refactoring plan. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config_list_from_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "            \n",
    "    # if clear_previous_work:\n",
    "    #     if os.path.exists('.coding') and os.path.isdir('.coding'):\n",
    "    #             print('deleting previous work...')\n",
    "    #             shutil.rmtree('.coding')\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# Dotenv with OPENAI_API_KEY\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='.env',\n",
    "    model_api_key_map={\n",
    "        # \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"huggingface/mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "        \"huggingface/glaiveai/glaive-coder-7b\": \"HUGGINGFACE_HUB\",\n",
    "\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            # \"gpt-4\",\n",
    "            \"huggingface/mistralai/Mistral-7B-v0.1\",\n",
    "            # \"huggingface/glaiveai/glaive-coder-7b\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.8,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png\")\n",
    "\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_cache():\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    folder_path = '.cache'\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        \n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# config_list_from_dotenv no OPENAI_API_KEY\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env',\n",
    "    model_api_key_map={\n",
    "        # \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        \"huggingface/mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            # \"gpt-4\",\n",
    "            \"huggingface/mistralai/Mistral-7B-v0.1\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock, save to png.\")\n",
    "\n",
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "            \n",
    "    # if clear_previous_work:\n",
    "    #     if os.path.exists('.coding') and os.path.isdir('.coding'):\n",
    "    #             print('deleting previous work...')\n",
    "    #             shutil.rmtree('.coding')\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "# Dotenv with OPENAI_API_KEY\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../.env',\n",
    "    model_api_key_map={\n",
    "        \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "        # \"huggingface/mistralai/Mistral-7B-v0.1\": \"HUGGINGFACE_HUB\",\n",
    "        # \"huggingface/glaiveai/glaive-coder-7b\": \"HUGGINGFACE_HUB\",\n",
    "\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            # \"huggingface/mistralai/Mistral-7B-v0.1\",\n",
    "            # \"huggingface/glaiveai/glaive-coder-7b\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "# print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.8,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png\")\n",
    "\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config_list_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def clear_cache():\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    folder_path = '.cache'\n",
    "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        \n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "clear_cache()\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='OAI_CONFIG_LIST',\n",
    "    filter_dict={\n",
    "            \"model\": {\n",
    "                \"huggingface/mistralai/Mistral-7B-v0.1\"\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "print(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg = lambda x: x.get(\"message\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot a chart to linechart.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Names vs. Config Names Discrepancy\n",
    "\n",
    "The names are not consistent across autogen and LiteLLM. This following code shows how to find the models from autogen that don't directly map to any model name in litellm and aims to do a closeness match, the final dictionary is then aggregated and added to completion.py\n",
    "\n",
    "*TODO: Make this automated in competion.py*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import litellm\n",
    "from litellm import completion\n",
    "print(autogen.__version__)\n",
    "\n",
    "print(type(autogen.Completion.chat_models))\n",
    "print(autogen.Completion.chat_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(autogen.Completion.price1K))\n",
    "print(autogen.Completion.price1K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(litellm.get_model_cost_map()))\n",
    "print(litellm.get_model_cost_map())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "from autogen.oai import Completion\n",
    "\n",
    "def map_oai_litellm_models():\n",
    "    \"\"\"\n",
    "    The purpose of this function is to find the corresponding\n",
    "    mappings of oai models as their names are different in litellm,\n",
    "    which we need to enable building agents with open sources models.\n",
    "\n",
    "    # Mapping\n",
    "    # For incommon models, a direct mapping can be created as they are named the same in both systems.\n",
    "    # For unique models, manual mapping might be needed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetching model data\n",
    "    litellm_models = set(litellm.get_model_cost_map().keys())  # Assuming it's a dict\n",
    "    autogen_models = set(Completion.price1K)  # Assuming it's an iterable of model names\n",
    "    \n",
    "    # Identifying common and unique models\n",
    "    common_models = autogen_models.intersection(litellm_models)\n",
    "    unique_autogen_models = autogen_models.difference(litellm_models)\n",
    "    unique_litellm_models = litellm_models.difference(autogen_models)\n",
    "    \n",
    "    # Mapping for common models\n",
    "    common_mapping = {model: model for model in common_models}\n",
    "    \n",
    "    # Mapping for unique models\n",
    "    unique_mapping = {}\n",
    "    for auto_model in unique_autogen_models:\n",
    "        closest_match = difflib.get_close_matches(auto_model, unique_litellm_models, n=2, cutoff=0.1)\n",
    "        if closest_match:\n",
    "            unique_mapping[auto_model] = closest_match[0]\n",
    "        else:\n",
    "            # Handle unmatched models. Potentially log them for manual mapping\n",
    "            unique_mapping[auto_model] = None\n",
    "            print(f\"No match found for {auto_model}. Consider manually mapping this model.\")\n",
    "    \n",
    "    # Merge mappings\n",
    "    model_mapping = {**common_mapping, **unique_mapping}\n",
    "    \n",
    "    return model_mapping\n",
    "\n",
    "model_mapping = map_oai_litellm_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "model_mapping.pop('code-cushman-001', None)\n",
    "pp.pprint(model_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us this to replace names to suit LiteLLM.\n",
    "litellm.model_alias_map = {\n",
    "    'code-davinci-002': 'davinci-002',\n",
    "    'gpt-3.5-turbo': 'gpt-3.5-turbo',\n",
    "    'gpt-3.5-turbo-0301': 'gpt-3.5-turbo-0301',\n",
    "    'gpt-3.5-turbo-0613': 'gpt-3.5-turbo-0613',\n",
    "    'gpt-3.5-turbo-16k': 'gpt-3.5-turbo-16k',\n",
    "    'gpt-3.5-turbo-16k-0613': 'gpt-3.5-turbo-16k-0613',\n",
    "    'gpt-35-turbo': 'gpt-3.5-turbo-instruct',\n",
    "    'gpt-4': 'gpt-4',\n",
    "    'gpt-4-0314': 'gpt-4-0314',\n",
    "    'gpt-4-0613': 'gpt-4-0613',\n",
    "    'gpt-4-32k': 'gpt-4-32k',\n",
    "    'gpt-4-32k-0314': 'gpt-4-32k-0314',\n",
    "    'gpt-4-32k-0613': 'gpt-4-32k-0613',\n",
    "    'text-ada-001': 'text-ada-001',\n",
    "    'text-babbage-001': 'text-babbage-001',\n",
    "    'text-curie-001': 'text-curie-001',\n",
    "    'text-davinci-002': 'davinci-002',\n",
    "    'text-davinci-003': 'text-davinci-003'\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
